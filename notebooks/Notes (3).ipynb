{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It was pointed out in [this notebook](https://www.kaggle.com/pestipeti/fake-incorrect-training-masks) that some of the images in the competition have masks that look artificial, pointing to potential false positives defensively inserted into the dataset:\n",
    "\n",
    "![](https://i.imgur.com/N8poUPx.png)\n",
    "\n",
    "* This led to some debate as to whether these entries should be included or excluded from training. Personally, after looking at these images for a bit I think some of these are false positives, but most are true, but very approximate, estimations of truths. For example, you can very faintly see the straitation being picked up as the presence of salt in the second image. A comment on the top geophysicist kernel on the competition implies that salt deposits can get very dirty and pick up characteristics of the surrounding rock, so I would believe it you told me you can have fault lines like this.\n",
    "\n",
    "  The test set for this competition also has images like this one. Probing has shown that excluding them reduces your public LB score.\n",
    "  \n",
    "  Overall point is that removing these images from consideration is an option that ought to be lightly explored.\n",
    "  \n",
    "  More: https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/64844\n",
    "  \n",
    "* The dataset has many empty masks. Predict whether or not an image has any salt in it at all, before moving onto the problem of finding the masks for the images which do have it (this comes heavily recommended).\n",
    "\n",
    "* The following set of tips are from [this forum thread](https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/63984):\n",
    "  * Unsupervised clustering (especially kNN) to group similar areas (and work on those?).\n",
    "  * Train a classifier to predict Kaggle metric score (huh?).\n",
    "  * Combine patches into bigger images.\n",
    "  * \"Pesudo-labeling, semi-suervised learning, knowledge distillation, adversarial training\"\n",
    "  * More labeling or supervisory signal (regularization by additional learning task), e.g create a distance transform regression target and train network to predict it.\n",
    "  * Make the problem multiclass (e.g. treat masks of different size, location, different image texture as different classes).\n",
    "  * Label train/sample as difficult or easy using prediction confidence. break big problems into smaller ones. difficult samples can have more post/pre processing.\n",
    "  * Further filter out very small pixel mask targets and work on those separately from big pixel mask targets.\n",
    "  * More suggestions in the [thread](https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/63984).\n",
    "  \n",
    "* Suggestions on preprocessing augmentations from [this forum thread](https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/63974):\n",
    "  * Randomly crop a 90% piece of the image and resize it to 128x128.\n",
    "  * Mirror the first few pixels on the border of the image outwards, and resize it to a 128x128 image (this results in CV improvement at a very high level).\n",
    "* Time-test augmentation, as discussed in [this article](https://towardsdatascience.com/augmentation-for-image-classification-24ffcbc38833).\n",
    "* The nets being used are pre-existing ones like ConvNet34, U-Net, etecetera. I need to read some of the associated papers to understand what the appropriate neural structures are for problems like this one.\n",
    "* Some inspirations in [this post](https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/64875).\n",
    "* Model comparisons: https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/64645\n",
    "* What loss function to use? Lovasz hinge. Per https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/64943.\n",
    "* Note to self: just start with doing selected recommended optimizations and passing the data through the U-Net in this example via a kernel: https://www.kaggle.com/phoenigs/u-net-dropout-augmentation-stratification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
